{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### faceID\n",
    "\n",
    "FaceID meaning one can register his/her face by taking few pictures of him and later use the FaceID to identify the person.\n",
    "\n",
    "One of the most discussed features of the new iPhone X is the new unlocking method, FaceID. Apple has revolutionized the way we unlock a phone: by simply looking at it. It uses front facing depth-camera for that. iPhone X in able to create a 3D map of the face of the user with that. In addition, a picture of the user’s face is captured using an infrared camera, that is more robust to changes in light and color of the environment. Using deep learning, the smartphone is able to learn the user face in great detail, thus recognizing him\\her every time the phone is picked up by its owner. \n",
    "\n",
    "Lets understand how it is different from the conventional face recognition system.\n",
    "Performing classification, for a neural network, means learning to predict if the face it has seen it’s the users’s one or not. So, it should use some training data to predict “true” or “false”. \n",
    "Here this approach would not work because it requires the network to be re-trained from scratch using the new obtained data from the user’s face. This would require a lot of time, energy consumption, and impractical availability of training data of different faces to have negative examples (little would change in case of transfer learning and fine tuning of an already trained network). \n",
    "\n",
    "Instead, FaceID is probably powered by a siamese-like convolutional neural network that is trained “offline” by Apple to map faces into a low-dimensional latent space shaped to maximize distances between faces of different people, using a contrastive loss. What happens is that you get an architecture capable of doing one shot learning.\n",
    "\n",
    "##### Siamese Network\n",
    "A siamese neural network is basically composed by two identical neural networks that also share all the weights. This architecture can learn to compute distances between particular kind of data, such as images. The idea is that you pass couples of data through the siamese networks (or simply pass the data in two different steps through the same network), the network maps it in a low dimensional feature space, like a n-dimensional array, and then you train the network to make this mapping so that data points from different classes are as far as possible, while data points from the same class are as close as possible. In the long run, the network will learn to extract the most meaningful features from data, and compress it into an array, creating an meaningful mapping. A siamese neural network can learn to do this for you, similarly to what an autoencoder does.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Start by Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iPhone X in able to create a 3D map of the face of the user using the front facing depth-camera. In addition, a picture of the user’s face is captured using an infrared camera, that is more robust to changes in light and color of the environment.\n",
    "\n",
    "For simplicity I have used only RGB images for the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -r faceid_train\n",
    "!rm -r faceid_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir faceid_train\n",
    "!mkdir faceid_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "link_list=[\"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-16)(151751).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-16)(153054).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-16)(154211).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-16)(160440).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-16)(160931).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-16)(161342).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-16)(163349).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-16)(164248).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-17)(141550).zip\", \\\n",
    "          \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-17)(142154).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-17)(142457).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-17)(143016).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(132824).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(133201).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(133846).zip\", \\\n",
    "          \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(134239).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(134757).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(140516).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(143345).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(144316).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(145150).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(145623).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(150303).zip\", \\\n",
    "          \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(150650).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(151337).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(151650).zip\"]\n",
    "val_list=[\"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(152717).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(153532).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(154129).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(154728).zip\", \"http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(155357).zip\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "for link in link_list:\n",
    "  r = requests.get(link, stream=True)\n",
    "  z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "  z.extractall(\"faceid_train\")\n",
    "for link in val_list:\n",
    "  r = requests.get(link, stream=True)\n",
    "  z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "  z.extractall(\"faceid_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import matplotlib.image as img\n",
    "\n",
    "os.chdir('D:\\\\Personal\\\\Practice Projects\\\\Computer Vision\\\\faceID\\\\Repo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create couples of images\n",
    "\n",
    "Siamese Neural Network is a special type of neural network in first we train an image with a sequence of convolutional layers, pooling layers and fully connected layers we end up with a feature vector f(x1).(See in Fig 1)\n",
    "\n",
    "Then we train another image in the same sequence to get another feature vector f(x2). Now we compute d which will be the distance between each of the points in feature vector f(x1) with the feature vector f(x2).\n",
    "\n",
    "If d is small we can tell both images are same else if d is large it’s the other way around.\n",
    "\n",
    "Some reference links for understanding Siamese Networks:\n",
    "https://www.youtube.com/watch?v=6jfw8MuKwpI\n",
    "https://medium.com/@subham.tiwari186/siamese-neural-network-for-one-shot-image-recognition-paper-analysis-44cf7f0c66cb\n",
    "\n",
    "So to implement this we will create two types of image couples.\n",
    "\n",
    "Right Couple - images of same person\n",
    "\n",
    "Wrong Couple - images of different persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[175., 161., 133.],\n",
       "         [173., 160., 133.],\n",
       "         [174., 158., 133.],\n",
       "         ...,\n",
       "         [151., 143., 129.],\n",
       "         [153., 143., 127.],\n",
       "         [153., 144., 124.]],\n",
       "\n",
       "        [[173., 160., 133.],\n",
       "         [173., 159., 133.],\n",
       "         [174., 158., 133.],\n",
       "         ...,\n",
       "         [153., 144., 128.],\n",
       "         [153., 141., 123.],\n",
       "         [154., 142., 122.]],\n",
       "\n",
       "        [[174., 158., 130.],\n",
       "         [174., 157., 130.],\n",
       "         [174., 158., 132.],\n",
       "         ...,\n",
       "         [154., 145., 124.],\n",
       "         [153., 141., 120.],\n",
       "         [152., 141., 122.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[148., 148., 160.],\n",
       "         [152., 149., 152.],\n",
       "         [157., 147., 139.],\n",
       "         ...,\n",
       "         [137., 147., 134.],\n",
       "         [136., 147., 136.],\n",
       "         [136., 147., 136.]],\n",
       "\n",
       "        [[149., 147., 156.],\n",
       "         [155., 147., 146.],\n",
       "         [158., 147., 131.],\n",
       "         ...,\n",
       "         [135., 147., 130.],\n",
       "         [134., 146., 133.],\n",
       "         [135., 147., 134.]],\n",
       "\n",
       "        [[149., 148., 151.],\n",
       "         [155., 147., 141.],\n",
       "         [159., 146., 129.],\n",
       "         ...,\n",
       "         [133., 146., 129.],\n",
       "         [134., 145., 129.],\n",
       "         [133., 147., 130.]]],\n",
       "\n",
       "\n",
       "       [[[177., 163., 142.],\n",
       "         [177., 163., 144.],\n",
       "         [177., 162., 146.],\n",
       "         ...,\n",
       "         [154., 149., 132.],\n",
       "         [155., 149., 130.],\n",
       "         [155., 149., 129.]],\n",
       "\n",
       "        [[176., 164., 141.],\n",
       "         [177., 164., 144.],\n",
       "         [177., 162., 145.],\n",
       "         ...,\n",
       "         [155., 150., 129.],\n",
       "         [156., 150., 128.],\n",
       "         [156., 149., 128.]],\n",
       "\n",
       "        [[175., 164., 142.],\n",
       "         [176., 163., 143.],\n",
       "         [176., 163., 143.],\n",
       "         ...,\n",
       "         [156., 150., 128.],\n",
       "         [157., 149., 128.],\n",
       "         [156., 148., 129.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[148., 151., 160.],\n",
       "         [151., 150., 154.],\n",
       "         [156., 150., 142.],\n",
       "         ...,\n",
       "         [139., 149., 135.],\n",
       "         [140., 149., 134.],\n",
       "         [139., 149., 136.]],\n",
       "\n",
       "        [[149., 149., 154.],\n",
       "         [154., 150., 146.],\n",
       "         [157., 150., 136.],\n",
       "         ...,\n",
       "         [138., 146., 133.],\n",
       "         [137., 148., 134.],\n",
       "         [136., 148., 134.]],\n",
       "\n",
       "        [[151., 149., 152.],\n",
       "         [156., 149., 142.],\n",
       "         [157., 149., 133.],\n",
       "         ...,\n",
       "         [134., 144., 131.],\n",
       "         [134., 146., 132.],\n",
       "         [135., 147., 130.]]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_couple_rgb(file_path):\n",
    "    \n",
    "    folder=np.random.choice(glob.glob(file_path + \"*\"))\n",
    "    \n",
    "    img = Image.open(np.random.choice(glob.glob(folder + \"/*.bmp\")))\n",
    "    img.thumbnail((640,480))\n",
    "    img = np.asarray(img)\n",
    "    img = img[140:340,220:420]\n",
    "    \n",
    "    img2 = Image.open(np.random.choice(glob.glob(folder + \"/*.bmp\")))\n",
    "    img2.thumbnail((640,480))\n",
    "    img2 = np.asarray(img2)\n",
    "    img2 = img2[140:340,220:420]\n",
    "    \n",
    "    full1 = np.zeros((200,200,3))\n",
    "    full1[:,:,:3] = img[:,:,:3]\n",
    "    \n",
    "    full2 = np.zeros((200,200,3))\n",
    "    full2[:,:,:3] = img2[:,:,:3]\n",
    "    \n",
    "    return np.array([full1, full2])\n",
    "\n",
    "create_couple_rgb(\"faceid_val/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_wrong_rgb(file_path):\n",
    "    folder=np.random.choice(glob.glob(file_path + \"*\"))\n",
    "    \n",
    "    img = Image.open(np.random.choice(glob.glob(folder + \"/*.bmp\")))\n",
    "    img.thumbnail((640,480))\n",
    "    img = np.asarray(img)\n",
    "    img = img[140:340,220:420]\n",
    "  #  plt.imshow(img)\n",
    "  #  plt.show()\n",
    "\n",
    "    folder2=np.random.choice(glob.glob(file_path + \"*\"))\n",
    "    while folder==folder2: #it activates if it chose the same folder\n",
    "        folder2=np.random.choice(glob.glob(file_path + \"*\"))\n",
    "    img2 = Image.open(np.random.choice(glob.glob(folder2 + \"/*.bmp\")))\n",
    "    img2.thumbnail((640,480))\n",
    "    img2 = np.asarray(img2)\n",
    "    img2 = img2[140:340,220:420]\n",
    " #   plt.imshow(img2)\n",
    " #   plt.show()\n",
    "    full1 = np.zeros((200,200,3))\n",
    "    full1[:,:,:3] = img[:,:,:3]\n",
    "    \n",
    "    full2 = np.zeros((200,200,3))\n",
    "    full2[:,:,:3] = img2[:,:,:3]\n",
    "    return np.array([full1, full2])\n",
    "\n",
    "create_wrong_rgb(\"faceid_val/\")[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "\n",
    "The model is a convolutional network  using Keras functional APIs based on the SqueezeNet architecture. The network takes as input RGB pictures of couples faces, so a 3 channel picture, and outputs a distance between the two embeddings. The network is trained with a constrastive loss, that minimizes distances between pictures of the same person and maximizes the distance between pictures of different persons.\n",
    "\n",
    "##### SqueezeNet \n",
    "\n",
    "For the same accuracy of AlexNet, SqueezeNet can be 3 times faster and 500 times smaller.\n",
    "\n",
    "The main ideas of SqueezeNet are:\n",
    "\n",
    "Using 1x1(point-wise) filters to replace 3x3 filters, as the former only 1/9 of computation.\n",
    "Using 1x1 filters as a bottleneck layer to reduce depth to reduce computation of the following 3x3 filters.\n",
    "Downsample late to keep a big feature map.\n",
    "\n",
    "The building brick of SqueezeNet is called fire module, which contains two layers: a squeeze layer and an expand layer. A SqueezeNet stackes a bunch of fire modules and a few pooling layers. The squeeze layer and expand layer keep the same feature map size, while the former reduce the depth to a smaller number, the later increase it. The squeezing (bottoleneck layer) and expansion behavior is common in neural architectures. Another common pattern is increasing depth while reducing feature map size to get high level abstract.\n",
    "\n",
    "SqueezeNet Reference\n",
    "https://github.com/DT42/squeezenet_demo\n",
    "\n",
    "##### Functional APIs in Keras\n",
    "\n",
    "The functional API in Keras is an alternate way of creating models that offers a lot more flexibility, including creating more complex models. \n",
    "\n",
    "It specifically allows you to define multiple input or output models as well as models that share layers. More than that, it allows you to define ad hoc acyclic network graphs.\n",
    "\n",
    "Models are defined by creating instances of layers and connecting them directly to each other in pairs, then defining a Model that specifies the layers to act as the input and output to the model.\n",
    "Keras Functional API\n",
    "https://keras.io/getting-started/functional-api-guide/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, Lambda, ELU, concatenate, GlobalAveragePooling2D, Input, BatchNormalization, SeparableConv2D, Subtract, concatenate\n",
    "from keras.activations import relu, softmax\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(inputs):\n",
    "    assert len(inputs) == 2, \\\n",
    "        'Euclidean distance needs 2 inputs, %d given' % len(inputs)\n",
    "    u, v = inputs\n",
    "    return K.sqrt(K.sum((K.square(u - v)), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true,y_pred):\n",
    "    margin=1.\n",
    "    return K.mean((1. - y_true) * K.square(y_pred) + y_true * K.square(K.maximum(margin - y_pred, 0.)))\n",
    "   # return K.mean( K.square(y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fire(x, squeeze=16, expand=64):\n",
    "    x = Convolution2D(squeeze, (1,1), padding='valid')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    left = Convolution2D(expand, (1,1), padding='valid')(x)\n",
    "    left = Activation('relu')(left)\n",
    "    \n",
    "    right = Convolution2D(expand, (3,3), padding='same')(x)\n",
    "    right = Activation('relu')(right)\n",
    "    \n",
    "    x = concatenate([left, right], axis=3)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200, 200, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 98, 98, 64)   4864        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 98, 98, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 98, 98, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 48, 48, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 48, 48, 16)   1040        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 48, 48, 16)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 48, 48, 16)   272         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 48, 48, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 48, 48, 16)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 48, 48, 16)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 48, 48, 32)   0           activation_3[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 23, 23, 32)   0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 23, 23, 32)   1056        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 23, 23, 32)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 23, 23, 32)   1056        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 23, 23, 32)   9248        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 23, 23, 32)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 23, 23, 32)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 23, 23, 64)   0           activation_6[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 11, 11, 64)   0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 11, 11, 48)   3120        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 11, 11, 48)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 11, 11, 48)   2352        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 11, 11, 48)   20784       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 11, 11, 48)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 11, 11, 48)   0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 11, 11, 96)   0           activation_9[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 11, 11, 96)   0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 11, 11, 512)  49664       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 11, 11, 512)  0           conv2d_11[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 96,032\n",
      "Trainable params: 95,904\n",
      "Non-trainable params: 128\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 200, 200, 3)       0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 11, 11, 512)       96032     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 61952)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               31719936  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 128)               0         \n",
      "=================================================================\n",
      "Total params: 31,881,632\n",
      "Trainable params: 31,881,504\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 200, 200, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 200, 200, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, 128)          31881632    input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1)            0           model_2[1][0]                    \n",
      "                                                                 model_2[2][0]                    \n",
      "==================================================================================================\n",
      "Total params: 31,881,632\n",
      "Trainable params: 31,881,504\n",
      "Non-trainable params: 128\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_input=Input(shape=(200,200,3))\n",
    "\n",
    "x = Convolution2D(64, (5, 5), strides=(2, 2), padding='valid')(img_input)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "x = fire(x, squeeze=16, expand=16)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "x = fire(x, squeeze=32, expand=32)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "\n",
    "x = fire(x, squeeze=48, expand=48)\n",
    "\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Convolution2D(512, (1, 1), padding='same')(x)\n",
    "out = Activation('relu')(x)\n",
    "\n",
    "\n",
    "modelsqueeze= Model(img_input, out)\n",
    "\n",
    "modelsqueeze.summary()\n",
    "\n",
    "im_in = Input(shape=(200,200,3))\n",
    "#wrong = Input(shape=(200,200,3))\n",
    "\n",
    "x1 = modelsqueeze(im_in)\n",
    "\n",
    "x1 = Flatten()(x1)\n",
    "\n",
    "x1 = Dense(512, activation=\"relu\")(x1)\n",
    "x1 = Dropout(0.2)(x1)\n",
    "#x1 = BatchNormalization()(x1)\n",
    "feat_x = Dense(128, activation=\"linear\")(x1)\n",
    "feat_x = Lambda(lambda  x: K.l2_normalize(x,axis=1))(feat_x)\n",
    "\n",
    "\n",
    "model_top = Model(inputs = [im_in], outputs = feat_x)\n",
    "\n",
    "model_top.summary()\n",
    "\n",
    "im_in1 = Input(shape=(200,200,3))\n",
    "im_in2 = Input(shape=(200,200,3))\n",
    "\n",
    "feat_x1 = model_top(im_in1)\n",
    "feat_x2 = model_top(im_in2)\n",
    "\n",
    "\n",
    "lambda_merge = Lambda(euclidean_distance)([feat_x1, feat_x2])\n",
    "\n",
    "model_final = Model(inputs = [im_in1, im_in2], outputs = lambda_merge)\n",
    "\n",
    "model_final.summary()\n",
    "\n",
    "adam = Adam(lr=0.001)\n",
    "\n",
    "sgd = SGD(lr=0.001, momentum=0.9)\n",
    "\n",
    "model_final.compile(optimizer=adam, loss=contrastive_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Generators\n",
    "Generators are especially useful for memory-intensive tasks, where there is no need to keep all of the elements of a memory-heavy list accessible at the same time. Calculating a series of values one-by-one can also be useful in situations where the complete result is never needed, yielding intermediate results to the caller until some requirement is satisfied and further processing stops.\n",
    "\n",
    "Reference : https://www.pythoncentral.io/python-generators-and-yield-keyword/\n",
    "\n",
    "###### yield statement\n",
    "\n",
    "The yield statement suspends function’s execution and sends a value back to caller, but retains enough state to enable function to resume where it is left off. When resumed, the function continues execution immediately after the last yield run. This allows its code to produce a series of values over time, rather them computing them at once and sending them back like a list.\n",
    "\n",
    "Reference: https://www.geeksforgeeks.org/use-yield-keyword-instead-return-keyword-python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(batch_size):\n",
    "  \n",
    "  while 1:\n",
    "    X=[]\n",
    "    y=[]\n",
    "    switch=True\n",
    "    for _ in range(batch_size):\n",
    "   #   switch += 1\n",
    "      if switch:\n",
    "     #   print(\"correct\")\n",
    "        X.append(create_couple_rgb(\"faceid_train/\").reshape((2,200,200,3)))\n",
    "        y.append(np.array([0.]))\n",
    "      else:\n",
    "     #   print(\"wrong\")\n",
    "        X.append(create_wrong_rgb(\"faceid_train/\").reshape((2,200,200,3)))\n",
    "        y.append(np.array([1.]))\n",
    "      switch=not switch\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    XX1=X[0,:]\n",
    "    XX2=X[1,:]\n",
    "    yield [X[:,0],X[:,1]],y\n",
    "\n",
    "def val_generator(batch_size):\n",
    "  \n",
    "  while 1:\n",
    "    X=[]\n",
    "    y=[]\n",
    "    switch=True\n",
    "    for _ in range(batch_size):\n",
    "      if switch:\n",
    "        X.append(create_couple_rgb(\"faceid_val/\").reshape((2,200,200,3)))\n",
    "        y.append(np.array([0.]))\n",
    "      else:\n",
    "        X.append(create_wrong_rgb(\"faceid_val/\").reshape((2,200,200,3)))\n",
    "        y.append(np.array([1.]))\n",
    "      switch=not switch\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    XX1=X[0,:]\n",
    "    XX2=X[1,:]\n",
    "    yield [X[:,0],X[:,1]],y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "20/20 [==============================] - 117s 6s/step - loss: 0.1373 - val_loss: 0.1803\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 117s 6s/step - loss: 0.1044 - val_loss: 0.4479\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 113s 6s/step - loss: 0.1069 - val_loss: 0.1662\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 116s 6s/step - loss: 0.0913 - val_loss: 0.1646\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 117s 6s/step - loss: 0.0849 - val_loss: 0.1173\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 129s 6s/step - loss: 0.0727 - val_loss: 0.1685\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 125s 6s/step - loss: 0.0811 - val_loss: 0.1084\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 120s 6s/step - loss: 0.0635 - val_loss: 0.0624\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 109s 5s/step - loss: 0.0611 - val_loss: 0.0649\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 111s 6s/step - loss: 0.0549 - val_loss: 0.0491\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 121s 6s/step - loss: 0.0535 - val_loss: 0.0768\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 120s 6s/step - loss: 0.0558 - val_loss: 0.2269\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 118s 6s/step - loss: 0.0553 - val_loss: 0.0684\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 115s 6s/step - loss: 0.0413 - val_loss: 0.0492\n",
      "Epoch 15/30\n",
      "20/20 [==============================] - 120s 6s/step - loss: 0.0471 - val_loss: 0.0633\n",
      "Epoch 16/30\n",
      "20/20 [==============================] - 123s 6s/step - loss: 0.0410 - val_loss: 0.0404\n",
      "Epoch 17/30\n",
      "20/20 [==============================] - 115s 6s/step - loss: 0.0416 - val_loss: 0.0920\n",
      "Epoch 18/30\n",
      "20/20 [==============================] - 110s 6s/step - loss: 0.0293 - val_loss: 0.0567\n",
      "Epoch 19/30\n",
      "20/20 [==============================] - 111s 6s/step - loss: 0.0371 - val_loss: 0.0590\n",
      "Epoch 20/30\n",
      "20/20 [==============================] - 120s 6s/step - loss: 0.0349 - val_loss: 0.0479\n",
      "Epoch 21/30\n",
      "20/20 [==============================] - 115s 6s/step - loss: 0.0307 - val_loss: 0.0465\n",
      "Epoch 22/30\n",
      "20/20 [==============================] - 115s 6s/step - loss: 0.0275 - val_loss: 0.0344\n",
      "Epoch 23/30\n",
      "20/20 [==============================] - 127s 6s/step - loss: 0.0306 - val_loss: 0.0275\n",
      "Epoch 24/30\n",
      "20/20 [==============================] - 134s 7s/step - loss: 0.0267 - val_loss: 0.0525\n",
      "Epoch 25/30\n",
      "20/20 [==============================] - 130s 6s/step - loss: 0.0350 - val_loss: 0.0576\n",
      "Epoch 26/30\n",
      "20/20 [==============================] - 125s 6s/step - loss: 0.0257 - val_loss: 0.0478\n",
      "Epoch 27/30\n",
      "20/20 [==============================] - 115s 6s/step - loss: 0.0264 - val_loss: 0.0538\n",
      "Epoch 28/30\n",
      "20/20 [==============================] - 108s 5s/step - loss: 0.0216 - val_loss: 0.0395\n",
      "Epoch 29/30\n",
      "20/20 [==============================] - 116s 6s/step - loss: 0.0220 - val_loss: 0.0488\n",
      "Epoch 30/30\n",
      "20/20 [==============================] - 113s 6s/step - loss: 0.0177 - val_loss: 0.0285\n"
     ]
    }
   ],
   "source": [
    "gen = generator(16)\n",
    "val_gen = val_generator(4)\n",
    "\n",
    "outputs = model_final.fit_generator(gen, steps_per_epoch=20, epochs=30, validation_data = val_gen, validation_steps=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 308ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08918415009975433"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cop = create_couple_rgb(\"faceid_val/\")\n",
    "model_final.evaluate([cop[0].reshape((1,200,200,3)), cop[1].reshape((1,200,200,3))], np.array([0.]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39827284]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cop = create_wrong_rgb(\"faceid_val/\")\n",
    "model_final.predict([cop[0].reshape((1,200,200,3)), cop[1].reshape((1,200,200,3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 200, 200, 3)       0         \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 128)               31881632  \n",
      "=================================================================\n",
      "Total params: 31,881,632\n",
      "Trainable params: 31,881,504\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00261367, -0.05340896, -0.1115457 ,  0.15691006, -0.04509634,\n",
       "        -0.01239945, -0.01408217, -0.10746031, -0.03028131,  0.04334357,\n",
       "        -0.20382701,  0.14370985, -0.08577099,  0.15171705, -0.07230415,\n",
       "        -0.04762426,  0.10633887,  0.11127307, -0.17736243, -0.07621372,\n",
       "        -0.09036388, -0.06888621, -0.06227786, -0.05857017,  0.09928378,\n",
       "        -0.047521  ,  0.11777283,  0.04829838,  0.1258049 ,  0.08454979,\n",
       "         0.09610505,  0.04089264,  0.00564953,  0.01705887, -0.18319975,\n",
       "         0.12174091, -0.06999657, -0.10190628, -0.10086499,  0.11602869,\n",
       "         0.0402711 ,  0.06525542, -0.10721307,  0.01282545, -0.02046915,\n",
       "         0.09345461,  0.00687979, -0.12160981, -0.04479368, -0.0487766 ,\n",
       "        -0.01854161,  0.04747349, -0.02052082,  0.01726987, -0.19819711,\n",
       "        -0.13587762,  0.06760719, -0.03664592, -0.15892474,  0.03253107,\n",
       "        -0.05126897, -0.03726244,  0.05253601,  0.02867259,  0.00363689,\n",
       "        -0.09252361, -0.20906316,  0.03837734,  0.04119583,  0.05299379,\n",
       "        -0.03326587,  0.04995766,  0.01601729, -0.13197443, -0.17356928,\n",
       "        -0.12914793,  0.00627051, -0.03630314, -0.09562187,  0.08313834,\n",
       "        -0.00660314,  0.08824458,  0.02129777,  0.07295372, -0.02716558,\n",
       "        -0.04643123, -0.13626003,  0.10407151,  0.02357841,  0.04178979,\n",
       "         0.06651901,  0.04223047,  0.02381893, -0.05922817, -0.10041541,\n",
       "         0.00927797,  0.05084047, -0.04648495,  0.04151585, -0.08802643,\n",
       "         0.14871742,  0.01484425, -0.01608207,  0.0066102 , -0.14197294,\n",
       "         0.05404851, -0.15899655,  0.05454305, -0.0434216 , -0.06921268,\n",
       "        -0.07392307,  0.00173246,  0.05983075, -0.09572487, -0.06821571,\n",
       "         0.07470945,  0.03100719,  0.04263184,  0.01665281, -0.09722291,\n",
       "        -0.02468522, -0.21462618, -0.17711467,  0.09986147, -0.08754632,\n",
       "         0.02818001,  0.05685977,  0.13242757]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "im_in1 = Input(shape=(200,200,3))\n",
    "#im_in2 = Input(shape=(200,200,4))\n",
    "\n",
    "feat_x1 = model_top(im_in1)\n",
    "#feat_x2 = model_top(im_in2)\n",
    "\n",
    "\n",
    "\n",
    "model_output = Model(inputs = im_in1, outputs = feat_x1)\n",
    "\n",
    "model_output.summary()\n",
    "\n",
    "adam = Adam(lr=0.001)\n",
    "\n",
    "sgd = SGD(lr=0.001, momentum=0.9)\n",
    "\n",
    "model_output.compile(optimizer=adam, loss=contrastive_loss)\n",
    "\n",
    "cop = create_couple_rgb(\"faceid_val/\")\n",
    "model_output.predict(cop[0].reshape((1,200,200,3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_input_rgb(file_path):\n",
    "    img = Image.open(file_path)\n",
    "    img.thumbnail((640,480))\n",
    "    img = np.asarray(img)\n",
    "    img = img[140:340,220:420]\n",
    "#    plt.figure(figsize=(8,8))\n",
    "#    plt.grid(True)\n",
    "#    plt.xticks([])\n",
    "#    plt.yticks([])\n",
    "#    plt.imshow(img)\n",
    "#    plt.show()\n",
    "  \n",
    "    full1 = np.zeros((200,200,3))\n",
    "    full1[:,:,:3] = img[:,:,:3]\n",
    "    \n",
    "    return np.array([full1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "Folder  1  of  14\n",
      "51\n",
      "Folder  2  of  14\n",
      "51\n",
      "Folder  3  of  14\n",
      "51\n",
      "Folder  4  of  14\n",
      "51\n",
      "Folder  5  of  14\n",
      "51\n",
      "Folder  6  of  14\n",
      "51\n",
      "Folder  7  of  14\n",
      "51\n",
      "Folder  8  of  14\n",
      "51\n",
      "Folder  9  of  14\n",
      "51\n",
      "Folder  10  of  14\n",
      "51\n",
      "Folder  11  of  14\n",
      "51\n",
      "Folder  12  of  14\n",
      "51\n",
      "Folder  13  of  14\n",
      "51\n",
      "Folder  14  of  14\n",
      "714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(714, 128)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "outputs=[]\n",
    "n=0\n",
    "for folder in glob.glob('faceid_train/*'):\n",
    "  i=0\n",
    "  for file in glob.glob(folder + '/*.bmp'):\n",
    "    i+=1\n",
    "    outputs.append(model_output.predict(create_input_rgb(file).reshape((1,200,200,3))))\n",
    "  print(i)\n",
    "  n+=1\n",
    "  print(\"Folder \", n, \" of \", len(glob.glob('faceid_train/*')))\n",
    "print(len(outputs))\n",
    "\n",
    "outputs= np.asarray(outputs)\n",
    "outputs = outputs.reshape((-1,128))\n",
    "outputs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(714, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_embedded = TSNE(2).fit_transform(outputs)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x196834a0ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color = 0\n",
    "for i in range(len((X_embedded))):\n",
    "  el = X_embedded[i]\n",
    "  if i % 51 == 0 and not i==0:\n",
    "    color+=1\n",
    "    color=color%10\n",
    "  plt.scatter(el[0], el[1], color=\"C\" + str(color))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between two different faces in Train Set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.7088729]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Distance between two different faces in Train Set\")\n",
    "\n",
    "file1 = ('faceid_train/(2012-05-16)(154211)/015_1_c.bmp')\n",
    "inp1 = create_input_rgb(file1)\n",
    "file1 = ('faceid_train/(2012-05-16)(153054)/004_1_c.bmp')\n",
    "inp2 = create_input_rgb(file1)\n",
    "\n",
    "model_final.predict([inp1, inp2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between two same faces in Validation Set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.05561978]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Distance between two same faces in Validation Set\")\n",
    "\n",
    "file1 = ('faceid_val/(2012-05-18)(152717)/015_1_c.bmp')\n",
    "inp1 = create_input_rgb(file1)\n",
    "file1 = ('faceid_val/(2012-05-18)(152717)/004_1_c.bmp')\n",
    "inp2 = create_input_rgb(file1)\n",
    "\n",
    "model_final.predict([inp1, inp2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between two different faces in Validation Set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.0902051]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Distance between two different faces in Validation Set\")\n",
    "\n",
    "file1 = ('faceid_val/(2012-05-18)(152717)/015_1_c.bmp')\n",
    "inp1 = create_input_rgb(file1)\n",
    "file1 = ('faceid_val/(2012-05-18)(153532)/004_1_c.bmp')\n",
    "inp2 = create_input_rgb(file1)\n",
    "\n",
    "model_final.predict([inp1, inp2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see the distance between similar faces is small and high for the different faces\n",
    "\n",
    "Retraining a network to perform classification, even fine-tuning one, is a complex thing to do especially on a mobile device. One has to provide the network with both positive examples (a small amount: the registration pictures) and negative examples (ideally, the huge dataset that Apple should have gathered) that is unfeasible on a mobile device.\n",
    "\n",
    "Recent work on Face Recognition using these techniques achieve incredible results (check out FaceNet), while being able to become a one-shot learning system (train once and for all)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
